# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lzMEMi5Mu_nIaT9sKpiZagwcdpt1ctAQ

Importing Modules
"""

import pandas as pd
df=pd.read_csv('projectdatasetfinal.csv')

"""Converting to lowercase"""

import nltk

"""Converting to Lower Case"""

import string
df['Abstract'] = [doc.lower() for doc in df['Abstract']]
print(df['Abstract'])

df['Title']= [doc.lower() for doc in df['Title']]
print(df['Title'])

df['Abstract']

"""Punctuation Removal"""

def remove_punctuations(text):
    for punctuation in string.punctuation:
        text = text.replace(punctuation, '')
    return text

df['Abstract']=df['Abstract'].apply(remove_punctuations)
df['Abstract']

!pip install wordnet

"""Lemmatization"""

import wordnet
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import pandas as pd

# Init the Wordnet Lemmatizer
lemmatizer = WordNetLemmatizer()
sentence = 'FINAL_KEYWORDS'
def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

"""Tokenization"""

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
df['Preprocessed'] = df['Abstract'].apply(lambda sentence: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])

"""Stop Word Removal"""

from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')
stop+=["already","also","make","made","hence","without","among","different","using","many","enables"]
df['Preprocessed']=df['Preprocessed'].apply(lambda x: [item for item in x if item not in stop])

"""Exporting normalized dataset"""

df.to_csv('cleaned_data.csv')

df['Preprocessed']